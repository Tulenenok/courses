{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве домашнего задания вам предлагается поработать над предсказанием погоды. Файл с данными вы найдете в соответствующей директории. Вам будет доступен датасет weather.csv, ПЕРВЫЕ 75% (shuffle = False) которого нужно взять для обучения, последние 25% - для тестирования.\n",
    "\n",
    "Требуется построить 4 модели которые будут предсказывать целевую переменную <b>RainTomorrow</b> с помощью:\n",
    "\n",
    "   1. логистической регрессии [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)\n",
    "   \n",
    "   2. метода ближайших соседей [sklearn.neighbors](https://scikit-learn.org/stable/modules/neighbors.html)\n",
    " \n",
    "   3. Байесовского классификатора [sklearn.naive_bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "   \n",
    "   4. логистической регрессии реализованной самостоятельно\n",
    "\n",
    "Затем следует сравнить результаты моделей (по качеству и времени выполнения) и сделать вывод о том, какая модель и с какими параметрами даёт лучшие результаты.\n",
    "\n",
    "Не забывайте о том, что работа с признаками играет очень большую роль в построении хорошей модели.\n",
    "\n",
    "Краткое описание данных:\n",
    "\n",
    "    Date - Дата наблюдений\n",
    "    Location - Название локации, в которой расположена метеорологическая станция\n",
    "    MinTemp - Минимальная температура в градусах цельсия\n",
    "    MaxTemp - Максимальная температура в градусах цельсия\n",
    "    Rainfall - Количество осадков, зафиксированных за день в мм\n",
    "    Evaporation - Так называемое \"pan evaporation\" класса А (мм) за 24 часа до 9 утра\n",
    "    Sunshine - Число солнечных часов за день\n",
    "    WindGustDir - направление самого сильного порыва ветра за последние 24 часа\n",
    "    WindGustSpeed - скорость (км / ч) самого сильного порыва ветра за последние 24 часа\n",
    "    WindDir9am - направление ветра в 9 утра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация логистической регрессии\n",
    "__Логистическая регрессия__\n",
    "\n",
    "$$p(y|x) = a(x, \\theta) = \\sigma(\\langle x, \\theta \\rangle) = \\frac{1}{1 + \\exp(-\\langle \\theta, x_i \\rangle)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([1, 2, 3])\n",
    "\n",
    "X =  np.array([[ 1,  1, 1],\n",
    "               [-1, -2, 1],\n",
    "               [-1, -2, 2],\n",
    "               [-2, -2, -3]\n",
    "              ])\n",
    "\n",
    "y = np.array([1, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(theta, X):\n",
    "    return 1 / (1 + np.exp(-np.dot(X, theta.T)))\n",
    "\n",
    "prob = probability(theta, X)\n",
    "\n",
    "assert type(prob) == np.ndarray, 'Возвращается неверный тип'\n",
    "assert prob.shape == (X.shape[0],), 'Неверный размер массива'\n",
    "assert (prob.round(3) == [0.998, 0.119, 0.731, 0.]).all(), 'Функция считается неверно'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.97527377e-01, 1.19202922e-01, 7.31058579e-01, 3.05902227e-07])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция предсказания метки класса, получает на вход вероятности принадлежности к классу 1 и выдает метки классов $y \\in \\{0, 1\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def binary_class_prediction(theta, X, threshold =.5):\n",
    "    return (probability(theta, X) > threshold).astype(int)\n",
    "\n",
    "y_pred = binary_class_prediction(theta, X)\n",
    "\n",
    "assert type(y_pred) == np.ndarray, 'Возвращается неверный тип'\n",
    "assert y_pred.shape == (X.shape[0],), 'Неверный размер массива'\n",
    "assert min(y_pred) == 0, 'Функция считается неверно'\n",
    "assert max(y_pred) == 1, 'Функция считается неверно'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Функционал качества логистической регрессии__\n",
    "\n",
    "Запишем правдободовие выборки для меток класса $y \\in \\{+1, -1\\}$ \n",
    "\n",
    "$$Likelihood(a, X^\\ell) = \\prod_{i = 1}^{\\ell} a(x_i,\\theta)^{[y_i = +1]} (1 - a(x_i, \\theta))^{[y_i = -1]} → \\operatorname*{max}_{\\theta}$$ \n",
    "\n",
    "Прологарифмируем правдоподобие выборки и перейдем к задаче минимизации:\n",
    "\n",
    "$$Q(a, X^\\ell) =     -\\sum_{i = 1}^{\\ell} \n",
    "        [y_i = +1] \\log a(x_i, \\theta)\n",
    "        +\n",
    "        [y_i = -1] \\log (1 - a(x_i, \\theta)) \\to \\operatorname*{min}_{\\theta}$$ \n",
    "        \n",
    "Подставим $a(x, \\theta)$ в функцинал качества:\n",
    "\n",
    "$$ Q(a, X^\\ell) = -\\sum_{i = 1}^{\\ell} \\left(\n",
    "    [y_i = +1]\n",
    "    \\log \\frac{1}{1 + \\exp(-\\langle \\theta, x_i \\rangle)}\n",
    "    +\n",
    "    [y_i = -1]\n",
    "    \\log \\frac{\\exp(-\\langle \\theta, x_i \\rangle)}{1 + \\exp(-\\langle \\theta, x_i \\rangle)}\n",
    "\\right)\n",
    "=\\\\\n",
    "=\n",
    "-\\sum_{i = 1}^{\\ell} \\left(\n",
    "    [y_i = +1]\n",
    "    \\log \\frac{1}{1 + \\exp(-\\langle \\theta, x_i \\rangle)}\n",
    "    +\n",
    "    [y_i = -1]\n",
    "    \\log \\frac{1}{1 + \\exp(\\langle \\theta, x_i \\rangle)}\n",
    "\\right)\n",
    "=\\\\\n",
    "=\n",
    "\\sum_{i = 1}^{\\ell}\n",
    "    \\log \\left(\n",
    "        1 + \\exp(-y_i \\langle \\theta, x_i \\rangle)\n",
    "    \\right) $$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговый оптимизируемый функционал качества (logloss), записанный для меток классов $y \\in \\{+1, -1\\}$ и усредненный по выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q(a, X^\\ell) = \\frac{1}{\\ell}\\sum_{i = 1}^{\\ell}\n",
    "    \\log \\left(\n",
    "        1 + \\exp(-y_i \\langle \\theta, x_i \\rangle)\n",
    "    \\right) \\to \\operatorname*{min}_{\\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем его в функции logloss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(theta, X, y): \n",
    "    y = np.where(y == 1, 1, -1)\n",
    "    return np.sum(np.log(1 + np.exp(-y * np.dot(X, theta.T)))) / y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8606664224002999"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logloss(theta, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert logloss(theta, X, y).round(3) == 0.861, 'Функция считается неверно'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Алгоритм оптимизации функционала качества. Стохастический градиентный спуск__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Вход: </b> Выборка $X^\\ell$, темп обучения $h$\n",
    "\n",
    "<b>Выход: </b> оптимальный вектор весов $\\theta$\n",
    "\n",
    "1.  Инициализировать веса $\\theta$\n",
    "2.  Инициализировать оценку функционала качества: $Q(a, X^\\ell)$\n",
    "3.  <b>Повторять</b>: \n",
    "\n",
    "    Выбрать случайным образом подвыборку объектов $X^{batch} =\\{x_1, \\dots,x_n \\}$ из $X^{\\ell}$\n",
    "    \n",
    "    Рассчитать градиент функционала качества: $\\nabla Q(X^{batch}, \\theta)$\n",
    "    \n",
    "    Обновить веса: $\\theta := \\theta - h\\cdot \\nabla Q(X^{batch}, \\theta)$\n",
    "       \n",
    "    <b>Пока</b> значение $Q$ и/или веса $\\theta$ не сойдутся   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем функцию рассчета градиента функционала качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial Q(a, X^{batch}) }{\\partial \\theta_j}   = \\frac{\\partial \\frac{1}{n}\\sum_{i = 1}^{n}\n",
    "    \\log \\left(\n",
    "        1 + \\exp(- y_i \\langle \\theta, x_i \\rangle)\n",
    "    \\right)} {\\partial \\theta_j}  = \\frac{1}{n}\\sum_{i = 1}^{n}\n",
    "     \\frac {1}{\n",
    "        1 + \\exp(- y_i \\langle \\theta, x_i \\rangle)} \\cdot  \\exp(- y_i \\langle \\theta, x_i \\rangle) \\cdot -y_i x_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте рассчет градиента в матричном виде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(theta, X, y):\n",
    "    result = np.zeros(len(theta))\n",
    "    for k in range(len(theta)):\n",
    "        result[k] = np.sum(np.exp(-y*X.dot(theta))/(1 + np.exp(-y*X.dot(theta)))*(-y*X[:, k]))/len(y)\n",
    "    \n",
    "    return result\n",
    "\n",
    "assert gradient(theta, X, y).shape == theta.shape, 'Неверный размер массива'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.21958111,  0.43978038, -0.22081743])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(theta, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция обучения уже реализована"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, batch_size=10, h=0.05,  iters=100, plot=True):\n",
    "\n",
    "    # получаем размерности матрицы\n",
    "    size, dim = X.shape\n",
    "\n",
    "    # случайная начальная инициализация\n",
    "    theta = np.random.uniform(size=dim)\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    theta_history = theta\n",
    "    colors = [plt.get_cmap('gist_rainbow')(i) for i in np.linspace(0,1,dim)]\n",
    "    \n",
    "    # plt \n",
    "    if plot:\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        ax1 = fig.add_subplot(221)\n",
    "        ax2 = fig.add_subplot(222)\n",
    "        ax3 = fig.add_subplot(212)\n",
    "        fig.suptitle('Gradient descent')\n",
    "        \n",
    "        \n",
    "    for _ in range(iters):  \n",
    "        \n",
    "        # берём случайный набор элементов\n",
    "        batch = np.random.choice(size, batch_size, replace=False)\n",
    "        X_batch = X.iloc[batch]\n",
    "        y_batch = y.iloc[batch]\n",
    "\n",
    "        # считаем производные\n",
    "        grad = gradient(theta, X_batch, y_batch)\n",
    "        \n",
    "        assert type(grad) == np.ndarray, 'неверный тип'\n",
    "        assert len(grad.shape) == 1, 'Необходимо вернуть одномерный вектор'\n",
    "        assert grad.shape[0] == len(theta), 'длина вектора должна быть равной количеству весов'\n",
    "        \n",
    "        \n",
    "        # Обновляем веса\n",
    "        \n",
    "        theta -= grad * h\n",
    "        \n",
    "        theta_history = np.vstack((theta_history, theta))\n",
    "        \n",
    "        # error\n",
    "        loss = logloss(theta, X, y)\n",
    "        errors.append(loss)\n",
    "        \n",
    "        if plot:\n",
    "            ax1.clear()            \n",
    "            ax1.scatter(range(dim), theta, label='Gradient solution')\n",
    "            ax1.legend(loc=\"upper left\")\n",
    "            ax1.set_title('theta')\n",
    "            ax1.set_ylabel(r'$\\bar \\beta$')\n",
    "            ax1.set_xlabel('weight ID')\n",
    "            \n",
    "            \n",
    "            ax2.plot(range(_+1), errors, 'g-')\n",
    "            ax2.set_title('logloss')\n",
    "            ax2.set_xlabel('itarations')\n",
    "            \n",
    "            ax3.plot(theta_history)\n",
    "            ax3.set_title('update theta')\n",
    "            ax3.set_ylabel('value')\n",
    "            ax3.set_xlabel('itarations')\n",
    "            time.sleep(0.05)\n",
    "            fig.canvas.draw()   \n",
    "            \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_theta = fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = binary_class_prediction(optimal_theta, X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
